In the /shared/evanroot directory:

I first replicated the paperâ€™s results by running main.py with the main dataset (mergedfeaturessep102.csv). That produced results identical to the paper which are in manifests_dir/paper_replication/results (varying numbers of classifiers, 1 and 11 being the models used in the paper). Similarly, some output plots (AUC, feature importance, etc.) from running main.py are also in manifests_dir/paper_replication.
I found bugs in the readPermissionsandActions scripts which caused several columns in the final dataset to be 0 (e.g. SMS, SENSORS, STORAGE). I fixed those scripts so that they produced the correct output, which is in the file manifests_dir/permsacts/perm-actions1to22.csv. I then merged those corrected columns with the main dataset using the merge_csvs.ipynb, and output the corrected final dataset in manifests_dir/paper_replication/corrected_permacts.csv. This should be the new dataset used to train and evaluate models in main.py.
Another thing to try could be creating new groups of permissions and actions. These new features might help create a more accurate model. In the file manifests_dir/xmls/testpermsacts.csv there are all of the permissions and actions that were read through when running the readPermissionsandActions scripts, so you might want to try parsing through those to figure out what different new groups could be.
P.s. new_permsacts_test.csv has far fewer permissions and actions (and honestly most of the ones in new_permsacts1to22.csv are app-specific ones that might be hard to use). I would recommend starting with the ones in new_permsacts_test.csv, and then potentially expanding to add even more groups from there.
